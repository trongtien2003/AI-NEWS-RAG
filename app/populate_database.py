import argparse
import os
import shutil
import json
import requests
import chromadb
import unicodedata
import nltk
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
from utils.get_embedding_function import get_embedding_function
from utils.get_ipv4_ip import get_windows_ip
from langchain_chroma import Chroma  
nltk.download('punkt')  # DÃ¹ng Ä‘á»ƒ chia theo cÃ¢u náº¿u cáº§n
CHROMA_PATH = "chroma"
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # Láº¥y thÆ° má»¥c gá»‘c cá»§a project
DATA_FILE = os.path.join(BASE_DIR, "news_crawler", "vnexpress_articles.json")
OLLAMA_URL = f"http://{get_windows_ip()}:11434/api/tags"
print(f"ğŸ”— Connecting to Ollama at: {OLLAMA_URL}")

def main():
    """Main function to load, process, and store AI news articles in ChromaDB."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--reset", action="store_true", help="Reset the database.")
    args = parser.parse_args()

    if args.reset:
        print("âœ¨ Clearing Database...")
        clear_database()

    if not check_ollama():
        print("âŒ Ollama is not running! Start it with `ollama serve` and retry.")
        return

    if not os.path.exists(DATA_FILE):
        print("âš ï¸ No JSON data file found! Please provide `vnexpress_articles.json`.")
        return

    documents = load_documents()
    chunks = split_documents(documents)
    add_to_chroma(chunks)


def check_ollama():
    """Check if Ollama is running before processing embeddings."""
    try:
        response = requests.get(OLLAMA_URL, timeout=5)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False


def clean_text(text):
    """Loáº¡i bá» kÃ½ tá»± khÃ´ng há»£p lá»‡, surrogate characters, khoáº£ng tráº¯ng thá»«a."""
    if not isinstance(text, str):
        return ""

    cleaned_text = ''.join(c for c in text if unicodedata.category(c) != 'Cs')  # Loáº¡i bá» kÃ½ tá»± lá»—i
    cleaned_text = unicodedata.normalize("NFKC", cleaned_text)  # Chuáº©n hÃ³a Unicode
    cleaned_text = cleaned_text.encode("utf-8", "ignore").decode("utf-8")  # MÃ£ hÃ³a UTF-8
    cleaned_text = " ".join(cleaned_text.split())  # XÃ³a khoáº£ng tráº¯ng thá»«a

    return cleaned_text


def load_documents():
    """Load AI-related news articles from JSON file."""
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        articles = json.load(f)

    documents = []
    for article in articles:
        cleaned_content = clean_text(article.get("content_en", ""))
        cleaned_description = clean_text(article.get("description_en", ""))
        cleaned_title = clean_text(article.get("title_en", ""))
        cleaned_url = clean_text(article.get("url", ""))
        cleaned_date = clean_text(article.get("date", ""))

        if not cleaned_content:
            continue  # Bá» qua bÃ i viáº¿t rá»—ng

        # Ná»‘i description vÃ o content
        combined_content = f"{cleaned_description}\n\n{cleaned_content}"

        metadata = {
            "title": cleaned_title,
            "url": cleaned_url,
            "date": cleaned_date,
        }

        documents.append(Document(page_content=combined_content, metadata=metadata))

    print(f"âœ… Loaded {len(documents)} articles after cleaning.")
    return documents


def split_by_paragraphs(text):
    """Chia vÄƒn báº£n theo Ä‘oáº¡n vÄƒn (paragraph)."""
    paragraphs = text.split("\n\n")  # Chia theo dáº¥u xuá»‘ng dÃ²ng kÃ©p
    return [p.strip() for p in paragraphs if len(p.strip()) > 100]  # Loáº¡i bá» Ä‘oáº¡n quÃ¡ ngáº¯n


def split_by_sentences(text, max_sentences=6):
    """Chia vÄƒn báº£n thÃ nh chunk theo tá»«ng cÃ¢u."""
    sentences = nltk.tokenize.sent_tokenize(text)
    chunks = [" ".join(sentences[i:i + max_sentences]) for i in range(0, len(sentences), max_sentences)]
    return chunks


def split_documents(documents: list[Document], method="recursive"):
    """Split articles into smaller chunks using different strategies."""
    split_chunks = []

    for doc in documents:
        text = doc.page_content

        if method == "recursive":
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=150,
                length_function=len,
                is_separator_regex=False,
            )
            chunks = text_splitter.split_text(text)

        elif method == "paragraph":
            chunks = split_by_paragraphs(text)

        elif method == "sentence":
            chunks = split_by_sentences(text, max_sentences=6)

        else:
            raise ValueError("Invalid chunking method. Use 'recursive', 'paragraph', or 'sentence'.")

        for chunk in chunks:
            split_chunks.append(Document(page_content=chunk, metadata=doc.metadata))

    print(f"âœ… {len(split_chunks)} chunks generated using {method} splitting.")
    return split_chunks


def add_to_chroma(chunks: list[Document]):
    """Add document chunks to ChromaDB with Ollama embeddings."""
    print("ğŸ” Initializing ChromaDB...")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())

    # Táº¡o ID cho tá»«ng chunk
    chunks_with_ids = calculate_chunk_ids(chunks)
    print(f"ğŸ“Œ Generated {len(chunks_with_ids)} chunks with unique IDs.")

    # Láº¥y danh sÃ¡ch ID hiá»‡n táº¡i trong database
    existing_items = db.get(include=[])
    existing_ids = set(existing_items["ids"])
    print(f"ğŸ“Š Number of existing documents in DB: {len(existing_ids)}")

    # Chá»‰ thÃªm cÃ¡c chunk chÆ°a cÃ³ trong database
    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata["id"] not in existing_ids]

    if new_chunks:
        print(f"ğŸ‘‰ Adding {len(new_chunks)} new documents...")
        
        # Debug ná»™i dung chunks má»›i
        for chunk in new_chunks[:5]:  # In thá»­ 5 chunk Ä‘áº§u
            print(f"ğŸ“Œ Chunk ID: {chunk.metadata['id']}, Content: {chunk.page_content[:100]}...")

        new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]

        # ThÃªm dá»¯ liá»‡u vÃ o database
        db.add_documents(new_chunks, ids=new_chunk_ids)

        # Táº¡o láº¡i káº¿t ná»‘i vá»›i database Ä‘á»ƒ Ä‘áº£m báº£o cáº­p nháº­t thÃ nh cÃ´ng
        db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())
        print("âœ… Database updated successfully!")

        # Kiá»ƒm tra láº¡i embeddings
        docs = db.get(include=['embeddings'])
        print(f"ğŸ” Retrieved {len(docs['ids'])} documents from ChromaDB.")

        # Debug 5 embeddings Ä‘áº§u tiÃªn
        for i in range(min(5, len(docs['ids']))):
            emb = docs['embeddings'][i]
            if emb is None:
                print(f"âŒ Embedding for doc {docs['ids'][i]} is None!")
            else:
                print(f"âœ… Embedding for doc {docs['ids'][i]}: {emb[:5]}... (truncated)")

    else:
        print("âœ… No new documents to add.")

def calculate_chunk_ids(chunks):
    """Generate unique IDs for document chunks based on URL and chunk index."""
    last_url_id = None
    current_chunk_index = 0

    for chunk in chunks:
        url = chunk.metadata.get("url", "unknown")
        current_url_id = url  

        if current_url_id == last_url_id:
            current_chunk_index += 1
        else:
            current_chunk_index = 0

        chunk.metadata["id"] = f"{current_url_id}:{current_chunk_index}"
        last_url_id = current_url_id

    return chunks


def clear_database():
    """Delete the existing ChromaDB folder."""
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
        print("ğŸ—‘ï¸ Database cleared successfully!")


if __name__ == "__main__":
    main()
