from selenium import webdriver
from selenium.webdriver.common.by import By
import time
import csv
import json
import re
from helper_functions import configure_driver
from deep_translator import GoogleTranslator

# Kh·ªüi t·∫°o Google Translate API
translator = GoogleTranslator(source="auto", target="en")

# Danh s√°ch t·ª´ vi·∫øt t·∫Øt c·∫ßn thay th·∫ø
abbreviation_dict = {
    "AI": "Tr√≠ tu·ªá nh√¢n t·∫°o",
    "IoT": "Internet v·∫°n v·∫≠t",
    "ML": "H·ªçc m√°y",
    "NLP": "X·ª≠ l√≠ ng√¥n ng·ªØ t·ª± nhi√™n"
}

def replace_abbreviations(text):
    if not text:
        return ""
    for abbr, full_form in abbreviation_dict.items():
        text = re.sub(rf'\b{abbr}\b', full_form, text, flags=re.IGNORECASE)
    return text

# File l∆∞u d·ªØ li·ªáu JSON
output_file = "vnexpress_articles.json"

# ƒê·ªçc d·ªØ li·ªáu c≈© ƒë·ªÉ tr√°nh crawl tr√πng
try:
    with open(output_file, "r", encoding="utf-8") as file:
        articles = json.load(file)
        crawled_urls = {article["url"] for article in articles}  # T·∫°o set ƒë·ªÉ ki·ªÉm tra nhanh
except (FileNotFoundError, json.JSONDecodeError):
    articles = []
    crawled_urls = set()

# ƒê·ªçc danh s√°ch URL t·ª´ file CSV
input_file = "vnexpress_links.csv"
urls = []
with open(input_file, "r", encoding="utf-8") as file:
    reader = csv.reader(file)
    next(reader)  # B·ªè qua header
    urls = [row[0] for row in reader if row[0] not in crawled_urls]  # L·ªçc b·ªè URL ƒë√£ crawl

if not urls:
    print("‚úÖ T·∫•t c·∫£ c√°c URL ƒë√£ ƒë∆∞·ª£c crawl. Kh√¥ng c√≥ g√¨ m·ªõi ƒë·ªÉ thu th·∫≠p.")
    exit()

# Kh·ªüi t·∫°o Selenium WebDriver
driver = configure_driver()

def wait_for_page_load(driver, timeout=30):
    start_time = time.time()
    while time.time() - start_time < timeout:
        state = driver.execute_script("return document.readyState")
        if state == "complete":
            return True
        time.sleep(1)
    print("‚ö†Ô∏è C·∫£nh b√°o: Trang ch∆∞a t·∫£i ho√†n to√†n sau th·ªùi gian ch·ªù!")
    return False

def translate_content(content):
    if not content:
        return ""
    content = replace_abbreviations(content)
    paragraphs = content.split('\n')
    translated_paragraphs = [translator.translate(p.strip()) for p in paragraphs if p.strip()]
    return '\n'.join(translated_paragraphs)

# B·∫Øt ƒë·∫ßu crawl t·ª´ng URL
for url in urls:
    try: 
        driver.get(url)
        if not wait_for_page_load(driver):
            print(f"‚è≥ Trang {url} c√≥ th·ªÉ ch∆∞a t·∫£i ho√†n to√†n!")
            continue
    except Exception as e:
        print(f"Time out khi t·∫£i {url}, l·ªói: {e}")
        continue
    
    try:
        title = driver.find_element(By.CLASS_NAME, "title-detail").text.strip()
        description = driver.find_element(By.CLASS_NAME, "description").text.strip()
        content = driver.find_element(By.CLASS_NAME, "fck_detail").text.strip()
        date = driver.find_element(By.CLASS_NAME, "date").text.strip()
        
        # Thay th·∫ø t·ª´ vi·∫øt t·∫Øt v√† d·ªãch
        title, description, content = map(replace_abbreviations, [title, description, content])
        title_en, description_en, content_en = map(translator.translate, [title, description, translate_content(content)])
        
        article = {
            "url": url,
            "title": title,
            "title_en": title_en,
            "description": description,
            "description_en": description_en,
            "content": content,
            "content_en": content_en,
            "date": date
        }
        
        articles.append(article)
        
        # Ghi d·ªØ li·ªáu v√†o file JSON sau m·ªói b√†i vi·∫øt
        with open(output_file, "w", encoding="utf-8") as file:
            json.dump(articles, file, ensure_ascii=False, indent=4)
        
        print(f"‚úÖ ƒê√£ thu th·∫≠p v√† l∆∞u: {url}")
    
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y d·ªØ li·ªáu t·ª´ {url}: {e}")

# ƒê√≥ng tr√¨nh duy·ªát
driver.quit()
print(f"üéØ ƒê√£ ho√†n th√†nh crawl v√† l∆∞u v√†o {output_file}")